{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How Language Models Work\n",
        "\n",
        "In this notebook we will be building a simple language model from scratch. We will be using the **Complete Works of Shakespeare** as our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and Encode Text\n",
        "\n",
        "In this code cell, we read in the text file containing the **Complete Works of Shakespeare**. First we load the text in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text length: 1115393\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "with open(\"data/input.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Text length: {len(text)}\")\n",
        "\n",
        "first_60 = text[:60]\n",
        "print(f\"{first_60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Text Encoding\n",
        "\n",
        "The first step is to convert the text into a numerical format. The simplest way to do this is to create a mapping between each character and a number.\n",
        "\n",
        "Here, we:\n",
        "- Extract all unique characters by converting the text into a Python `set`.\n",
        "- Sort them so that we have a consistent ordering of characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "| |!|$|&|'|,|-|.|3|:|;|?|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z\n",
            "Unique characters: 65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print('|'.join(chars))\n",
        "print(f\"Unique characters: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our text there are 65 unique characters (26 uppercae, 26 lowercase and 13 numbers and special characters). So we can simply convert each character to a number from 1 to 65.\n",
        "\n",
        "In our case we'll sort alphabetically and then convert each character to a number. To do this we define two main dictionaries:\n",
        "- **`stoi` (string-to-integer)**: Maps each character to an integer.\n",
        "- **`itos` (integer-to-string)**: Maps back from the integer to the character.\n",
        "\n",
        "We then define two lambda functions:\n",
        "- `encode(s)`: Converts a string into a list of integer indices.\n",
        "- `decode(l)`: Converts a list of indices back into a string.\n",
        "\n",
        "Below are the first 60 characters encoded as integers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8]\n"
          ]
        }
      ],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(first_60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Splitting and Context-Target Pairs\n",
        "\n",
        "1. We **split** the data into a training set (90%) and a validation set (10%).\n",
        "2. We set a `block_size` of 8, meaning our context window includes 8 tokens (characters).\n",
        "3. We illustrate how each token in the block is used to predict the *next* token.\n",
        "\n",
        "In practice, the training loop will sample chunks of the text in random order, so the model sees a variety of patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text))\n",
        "\n",
        "# split into train and validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "block_size = 8\n",
        "\n",
        "# 9 items will have 8 predicion examples\n",
        "train_data[:block_size+1]\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "print(x)\n",
        "\n",
        "# useful so the transformer is used to seeing different lengths of data\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target is {target}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a Simple Bigram Language Model\n",
        "\n",
        "Here, we call the following classes:\n",
        "- **`BatchLoader`**: A small class to handle data in mini-batches.\n",
        "- **`Evaluator`**: A helper to measure perplexity on training/validation sets.\n",
        "- **`Trainer`**: Orchestrates the training loop.\n",
        "\n",
        "We instantiate `SimpleBigramLanguageModel` with:\n",
        "- `block_size = 16`\n",
        "- `batch_size = 32`\n",
        "\n",
        "We run up to `max_iters = 4001` steps, checking perplexity every `eval_interval = 500` steps.\n",
        "\n",
        "A final perplexity of ~11.8 means that for this model, it guesses the right next character about 1 in 12 times. Definitely an improvement from pure randomness, but still quite high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: perplexity: 112.3, \n",
            "step 500: perplexity: 27.1, \n",
            "step 1000: perplexity: 15.6, \n",
            "step 1500: perplexity: 13.1, \n",
            "step 2000: perplexity: 12.4, \n",
            "step 2500: perplexity: 12.1, \n",
            "step 3000: perplexity: 11.9, \n",
            "step 3500: perplexity: 11.9, \n",
            "step 4000: perplexity: 11.8, \n"
          ]
        }
      ],
      "source": [
        "from src import SimpleBigramLanguageModel, BatchLoader, Evaluator, Trainer\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 16\n",
        "max_iters = 4001\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-3\n",
        "\n",
        "# Setup data and model\n",
        "torch.manual_seed(1337)\n",
        "train_loader = BatchLoader(train_data, block_size=block_size, batch_size=batch_size)\n",
        "val_loader = BatchLoader(val_data, block_size=block_size, batch_size=batch_size)\n",
        "\n",
        "# model = SimpleBigramLanguageModel(vocab_size, n_embed, block_size)\n",
        "model = SimpleBigramLanguageModel(vocab_size, block_size)\n",
        "\n",
        "# Setup training components\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "evaluator = Evaluator(model, train_loader, val_loader, vocab_size)\n",
        "trainer = Trainer(model, optimizer, train_loader, evaluator, max_iters, eval_interval)\n",
        "\n",
        "# Train the model\n",
        "final_losses = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generated Text (Simple Bigram Model)\n",
        "\n",
        "Using `model.generate()`, we sample from our learned distribution to produce text. This snippet:\n",
        "- Creates an **empty** context (a single zero token)\n",
        "- Asks the model for the next 490 characters.\n",
        "- Decodes the token IDs back to characters.\n",
        "\n",
        "The text is somewhat \"Shakespeare-like\" but still full of nonsense. This is expected for a bigram character-level model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Wadoust ftes inupuctararirtowir gs wingucr, as aith helpr;\n",
            "Ju ove d rangos blthiok\n",
            "Pl ghese tringhan then ande\n",
            "\n",
            "I cus, Bu d:\n",
            "TEENGOKENCILA iby ELOLoristhe,\n",
            "Plethatird fu$go tckid t y ollf ta he cere. G ha hearee d ld beat gu bean ane as tiorseade marethioathowow alr, t ot anand are rend.\n",
            "LEOLUENTITENCKICOMEOLLLI e.\n",
            "KEThof aren:\n",
            "NIVI miecohyor, erinst flawhant fe ere bons thand athe it ilee-OR f: in, he spoth,\n",
            "Melladalee isaloveen ol it gllinde me h chin oug--ccas hed as o tot thit ance\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "generated_text = decode(model.generate(context, max_new_tokens=490)[0].tolist())\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introducing Self-Attention\n",
        "\n",
        "Here, we switch to a more advanced architecture: **`BigramLanguageModel`** that implements a simplified version of self-attention. Key differences:\n",
        "- We have `n_heads = 4`, meaning we use multi-head attention.\n",
        "- `dropout = 0.1` helps prevent overfitting.\n",
        "- `n_embed = 64` increases the dimension of our embeddings, letting the model learn more nuanced patterns.\n",
        "\n",
        "During training, you can see the perplexity now **drops faster** and much lower than the simple bigram model—down to around 6.1. This demonstrates the power of attention-based layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: perplexity: 66.7, \n",
            "step 500: perplexity: 8.5, \n",
            "step 1000: perplexity: 7.5, \n",
            "step 1500: perplexity: 7.0, \n",
            "step 2000: perplexity: 6.8, \n",
            "step 2500: perplexity: 6.6, \n",
            "step 3000: perplexity: 6.4, \n",
            "step 3500: perplexity: 6.3, \n",
            "step 4000: perplexity: 6.3, \n"
          ]
        }
      ],
      "source": [
        "from src import BigramLanguageModel\n",
        "\n",
        "# Model parameters\n",
        "batch_size = 32\n",
        "block_size = 16\n",
        "max_iters = 4001\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-3\n",
        "n_embed = 64\n",
        "n_heads = 4\n",
        "n_layer = 1\n",
        "dropout = 0.1\n",
        "#\n",
        "\n",
        "# model = SimpleBigramLanguageModel(vocab_size, n_embed, block_size)\n",
        "model = BigramLanguageModel(vocab_size, n_embed, block_size, n_layer, n_heads, dropout)\n",
        "\n",
        "# Setup training components\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "evaluator = Evaluator(model, train_loader, val_loader, vocab_size)\n",
        "trainer = Trainer(model, optimizer, train_loader, evaluator, max_iters, eval_interval)\n",
        "\n",
        "# Train the model\n",
        "final_losses = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generated Text (Self-Attention Model)\n",
        "\n",
        "Now that we have introduced self-attention, the output text, while still nonsensical in parts, contains more coherent words and partial sentences.\n",
        "\n",
        "You can see phrases like *\"your bes, you as:\"* and *\"Sher.'t Caren:\"* that—though random—are starting to look more like 16th-century English.\n",
        "\n",
        "By adjusting hyperparameters (e.g., more layers, larger embeddings), we can push this further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "HESS Nairs, as grown, fatendam trongoon body honour cinsfel spother dee\n",
            "Casens for would would his, joy banderve mawn that all mades,\n",
            "Or, in, fair io findance;\n",
            "So mell, Thees by ken denhower chies.\n",
            "\n",
            "EDWARWICK:\n",
            "Her coldwick owy titry\n",
            "Are, prikens to Prolost: Iuble your fom us:\n",
            "Your thate see, but hav\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "generated_text = decode(model.generate(context, max_new_tokens=300)[0].tolist())\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling Up Further\n",
        "\n",
        "To demonstrate the impact of scaling, we adjust:\n",
        "- **`block_size`** to a smaller 6 here (just for demonstration)\n",
        "- **`n_embed`** to 192\n",
        "- **`n_layer`** to 4\n",
        "- **`n_heads`** to 3\n",
        "\n",
        "Even with these changes, perplexity drops into the mid-5 range, showing how deeper networks and bigger embeddings improve predictive power. In practice, you'd also raise the `block_size` to allow the model to see longer context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: perplexity: 290.1, \n",
            "step 500: perplexity: 7.6, \n",
            "step 1000: perplexity: 6.7, \n",
            "step 1500: perplexity: 6.3, \n",
            "step 2000: perplexity: 6.2, \n",
            "step 2500: perplexity: 5.9, \n",
            "step 3000: perplexity: 5.7, \n",
            "step 3500: perplexity: 5.6, \n",
            "step 4000: perplexity: 5.5, \n",
            "step 4500: perplexity: 5.5, \n",
            "step 5000: perplexity: 5.4, \n",
            "step 5500: perplexity: 5.4, \n",
            "step 6000: perplexity: 5.3, \n",
            "step 6500: perplexity: 5.2, \n",
            "step 7000: perplexity: 5.1, \n",
            "step 7500: perplexity: 5.2, \n",
            "step 8000: perplexity: 5.1, \n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "batch_size = 32\n",
        "block_size = 16\n",
        "max_iters = 8001\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-3\n",
        "n_embed = 192\n",
        "n_heads = 3\n",
        "n_layer = 3\n",
        "dropout = 0.1\n",
        "#\n",
        "\n",
        "# model = SimpleBigramLanguageModel(vocab_size, n_embed, block_size)\n",
        "model = BigramLanguageModel(vocab_size, n_embed, block_size, n_layer, n_heads, dropout)\n",
        "\n",
        "# Setup training components\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "evaluator = Evaluator(model, train_loader, val_loader, vocab_size)\n",
        "trainer = Trainer(model, optimizer, train_loader, evaluator, max_iters, eval_interval)\n",
        "\n",
        "# Train the model\n",
        "final_losses = trainer.train()\n",
        "\n",
        "# 2.09 5.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Final Text Generation\n",
        "\n",
        "Our scaled-up model now produces text that hints at characters, place names, and partial phrases resembling Shakespeare.\n",
        "We see references to _\"LADY CAPULET\"_ and partial coherent lines that mimic stage directions or dialogues. While still not perfect English, it's closer in style to Shakespeare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "It's of it, Bolingbrokd say me\n",
            "Was my let little?\n",
            "\n",
            "PRINCE ELIZABETH:\n",
            "I, andeed Isabelgarern face, boy:\n",
            "Tyknow't hair, Venousin, that death know Klaint us hoy are hope's hope and will-harlst.\n",
            "What'struct by intale\n",
            "The rihour cory or than an,,\n",
            "Not justice precition 'tis the nuch and eyes?\n",
            "\n",
            "DUKE OF UntOLYCUS:\n",
            "Younharis!\n",
            "\n",
            "DUSHERDOLIO:\n",
            "The divicers.\n",
            "Upone this my throng and reat. No: even our me.\n",
            "They \n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "generated_text = decode(model.generate(context, max_new_tokens=400)[0].tolist())\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Cloud GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dannybellion/Documents/repos/github portfolio/How LLMs Work/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Syncing files to RunPod...\n"
          ]
        },
        {
          "ename": "AuthenticationError",
          "evalue": "Unauthorized request, please check your API key.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# First, sync files\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyncing files to RunPod...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m sync_success \u001b[38;5;241m=\u001b[39m \u001b[43msync_to_pod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/repos/github portfolio/How LLMs Work/src/cloud_utils.py:54\u001b[0m, in \u001b[0;36msync_to_pod\u001b[0;34m(pod_id)\u001b[0m\n\u001b[1;32m     50\u001b[0m client \u001b[38;5;241m=\u001b[39m RunPodClient()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pod_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Get all pods and find the active one\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     pods \u001b[38;5;241m=\u001b[39m \u001b[43mrunpod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pods\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound pods:\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(pods, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     57\u001b[0m     active_pods \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pods \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesiredStatus\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRUNNING\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m~/Documents/repos/github portfolio/How LLMs Work/.venv/lib/python3.12/site-packages/runpod/api/ctl_commands.py:74\u001b[0m, in \u001b[0;36mget_pods\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pods\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    Get all pods\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     raw_return \u001b[38;5;241m=\u001b[39m \u001b[43mrun_graphql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpod_queries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQUERY_POD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     cleaned_return \u001b[38;5;241m=\u001b[39m raw_return[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmyself\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpods\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_return\n",
            "File \u001b[0;32m~/Documents/repos/github portfolio/How LLMs Work/.venv/lib/python3.12/site-packages/runpod/api/graphql.py:35\u001b[0m, in \u001b[0;36mrun_graphql_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     32\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTP_STATUS_UNAUTHORIZED:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mAuthenticationError(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnauthorized request, please check your API key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson():\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mQueryError(response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m], query)\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Unauthorized request, please check your API key."
          ]
        }
      ],
      "source": [
        "import json\n",
        "from src.cloud_utils import sync_to_pod, run_on_pod\n",
        "\n",
        "# First, sync files\n",
        "print(\"Syncing files to RunPod...\")\n",
        "sync_success = sync_to_pod()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up training configuration...\n",
            "Error: Your SSH client doesn't support PTY\n",
            "Writing configuration...\n",
            "Running command: cd ~/shakespeare && python cloud_train.py config.json\n",
            "Error: Your SSH client doesn't support PTY\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define your configuration\n",
        "config = {\n",
        "    \"batch_size\": 32, #128\n",
        "    \"block_size\": 16, #512\n",
        "    \"max_iters\": 10000, #10000\n",
        "    \"eval_interval\": 500, #500\n",
        "    \"learning_rate\": 1e-4, #1e-4\n",
        "    \"n_embed\": 64, #512\n",
        "    \"n_heads\": 4, #8\n",
        "    \"n_layer\": 1,\n",
        "    \"dropout\": 0.2\n",
        "}\n",
        "\n",
        "# Convert config to JSON string\n",
        "config_str = json.dumps(config)\n",
        "\n",
        "# Install requirements\n",
        "# print(\"Installing requirements...\")\n",
        "# run_remote_command(ssh, \"cd shakespeare && pip install torch numpy tqdm\")\n",
        "\n",
        "# Start training with config\n",
        "print(\"\\nStarting training...\")\n",
        "run_on_pod(f'cd shakespeare && python cloud_train.py \\'{json.dumps(config)}\\'')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
