{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text length: 1115393\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "with open(\"data/input.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Text length: {len(text)}\")\n",
        "\n",
        "first_60 = text[:60]\n",
        "print(f\"{first_60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and Previewing the Text\n",
        "\n",
        "In this code cell, we read in the text file containing the **Complete Works of Shakespeare**. We then:\n",
        "- Print out the total length of the text.\n",
        "- Print the first 60 characters.\n",
        "\n",
        "This provides a quick sanity check to ensure the data is loaded and also gives us insight into the text structure and content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "| |!|$|&|'|,|-|.|3|:|;|?|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z\n",
            "Unique characters: 65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print('|'.join(chars))\n",
        "print(f\"Unique characters: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identifying the Character Set\n",
        "\n",
        "Here, we:\n",
        "- Extract all unique characters by converting the text into a Python `set`.\n",
        "- Sort them so that we have a consistent ordering of characters.\n",
        "- Print them to see exactly which characters appear in the text.\n",
        "- Print the total count, which in this case is 65.\n",
        "\n",
        "For this project, each **character** maps to a single integer. In modern LLMs, this mapping happens at a larger token (sub-word) level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8]\n"
          ]
        }
      ],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(first_60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding and Decoding\n",
        "\n",
        "We define two main dictionaries:\n",
        "- **`stoi` (string-to-integer)**: Maps each character to an integer.\n",
        "- **`itos` (integer-to-string)**: Maps back from the integer to the character.\n",
        "\n",
        "We then define two lambda functions:\n",
        "- `encode(s)`: Converts a string into a list of integer indices.\n",
        "- `decode(l)`: Converts a list of indices back into a string.\n",
        "\n",
        "Finally, we print the integer-encoded version of the first 60 characters to verify our mapping is working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text))\n",
        "\n",
        "# split into train and validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "block_size = 8\n",
        "\n",
        "# 9 items will have 8 predicion examples\n",
        "train_data[:block_size+1]\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "print(x)\n",
        "\n",
        "# useful so the transformer is used to seeing different lengths of data\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target is {target}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Splitting and Context-Target Pairs\n",
        "\n",
        "1. We **split** the data into a training set (90%) and a validation set (10%).\n",
        "2. We set a `block_size` of 8, meaning our context window includes 8 tokens (characters).\n",
        "3. We illustrate how each token in the block is used to predict the *next* token.\n",
        "\n",
        "In practice, the training loop will sample chunks of the text in random order, so the model sees a variety of patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: perplexity: 112.3, \n",
            "step 500: perplexity: 27.1, \n",
            "step 1000: perplexity: 15.6, \n",
            "step 1500: perplexity: 13.1, \n",
            "step 2000: perplexity: 12.4, \n",
            "step 2500: perplexity: 12.1, \n",
            "step 3000: perplexity: 11.9, \n",
            "step 3500: perplexity: 11.9, \n",
            "step 4000: perplexity: 11.8, \n",
            "step 4500: perplexity: 11.8, \n",
            "step 5000: perplexity: 11.7, \n"
          ]
        }
      ],
      "source": [
        "from src import SimpleBigramLanguageModel, BatchLoader, Evaluator, Trainer\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 16\n",
        "max_iters = 4500\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-3\n",
        "\n",
        "# Setup data and model\n",
        "torch.manual_seed(1337)\n",
        "train_loader = BatchLoader(train_data, block_size=block_size, batch_size=batch_size)\n",
        "val_loader = BatchLoader(val_data, block_size=block_size, batch_size=batch_size)\n",
        "\n",
        "# model = SimpleBigramLanguageModel(vocab_size, n_embed, block_size)\n",
        "model = SimpleBigramLanguageModel(vocab_size, block_size)\n",
        "\n",
        "# Setup training components\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "evaluator = Evaluator(model, train_loader, val_loader, vocab_size)\n",
        "trainer = Trainer(model, optimizer, train_loader, evaluator, max_iters, eval_interval)\n",
        "\n",
        "# Train the model\n",
        "final_losses = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a Simple Bigram Language Model\n",
        "\n",
        "Here, we define:\n",
        "- **`BatchLoader`**: A small class to handle data in mini-batches.\n",
        "- **`Evaluator`**: A helper to measure perplexity on training/validation sets.\n",
        "- **`Trainer`**: Orchestrates the training loop.\n",
        "\n",
        "We instantiate `SimpleBigramLanguageModel` with:\n",
        "- `block_size = 16`\n",
        "- `batch_size = 32`\n",
        "\n",
        "We run up to `max_iters = 4500` steps, checking perplexity every `eval_interval = 500` steps.\n",
        "\n",
        "A final perplexity of ~11.7 means that for this model, it guesses the right next character about 1 in 11.7 times. Definitely an improvement from pure randomness, but still quite high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "O:\n",
            "Anele er fe co,\n",
            "LLamer squsethaittthtr ayit tifod rer; y e ined guratosoulyequg.\n",
            "BUEEd tavaperelee athavis u warray, n\n",
            "We by bronond man, d cr miowivero agarlan\n",
            "has,\n",
            "\n",
            "Binksue; ain'lilavealeamy y t Isoup uge o'sth r.\n",
            "What Beeethisunded orachigorsh kn, Ta cheneinhit we t,\n",
            "Fr s ide Bus ithikee me;\n",
            "Bul ake har apy ave I arillevVIO hineeo n:\n",
            "TI ad by andulcavis, scld\n",
            "Atlithe day;\n",
            "\n",
            "AO: T:\n",
            "G butor benkeave y'd,\n",
            "Gecknfime ttinthalond sBy wapiorasonou haverl the heayet asen d bor t man pe, t\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "generated_text = decode(model.generate(context, max_new_tokens=490)[0].tolist())\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generated Text (Simple Bigram Model)\n",
        "\n",
        "Using `model.generate()`, we sample from our learned distribution to produce text. This snippet:\n",
        "- Creates an **empty** context (a single zero token)\n",
        "- Asks the model for the next 490 characters.\n",
        "- Decodes the token IDs back to characters.\n",
        "\n",
        "The text is somewhat \"Shakespeare-like\" but still full of nonsense. This is expected for a bigram character-level model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Attention Section (Empty Code Cell)\n",
        "\n",
        "This empty cell was part of the original notebook structure. It's a placeholder where one might add additional code or placeholders for analysis. We'll keep it here for continuity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: perplexity: 65.9, \n",
            "step 500: perplexity: 8.7, \n",
            "step 1000: perplexity: 7.6, \n",
            "step 1500: perplexity: 7.1, \n",
            "step 2000: perplexity: 6.8, \n",
            "step 2500: perplexity: 6.6, \n",
            "step 3000: perplexity: 6.4, \n",
            "step 3500: perplexity: 6.4, \n",
            "step 4000: perplexity: 6.2, \n",
            "step 4500: perplexity: 6.2, \n",
            "step 5000: perplexity: 6.1, \n"
          ]
        }
      ],
      "source": [
        "from src import BigramLanguageModel\n",
        "\n",
        "# Model parameters\n",
        "batch_size = 32\n",
        "block_size = 16\n",
        "max_iters = 4001\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-3\n",
        "n_embed = 64\n",
        "n_heads = 4\n",
        "n_layer = 1\n",
        "dropout = 0.1\n",
        "#\n",
        "\n",
        "# model = SimpleBigramLanguageModel(vocab_size, n_embed, block_size)\n",
        "model = BigramLanguageModel(vocab_size, n_embed, block_size, n_layer, n_heads, dropout)\n",
        "\n",
        "# Setup training components\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "evaluator = Evaluator(model, train_loader, val_loader, vocab_size)\n",
        "trainer = Trainer(model, optimizer, train_loader, evaluator, max_iters, eval_interval)\n",
        "\n",
        "# Train the model\n",
        "final_losses = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introducing Self-Attention\n",
        "\n",
        "Here, we switch to a more advanced architecture: **`BigramLanguageModel`** that implements a simplified version of self-attention. Key differences:\n",
        "- We have `n_heads = 4`, meaning we use multi-head attention.\n",
        "- `dropout = 0.1` helps prevent overfitting.\n",
        "- `n_embed = 64` increases the dimension of our embeddings, letting the model learn more nuanced patterns.\n",
        "\n",
        "During training, you can see the perplexity now **drops faster** and much lower than the simple bigram model—down to around 6.1. This demonstrates the power of attention-based layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "your bes, you as: nowful hear of nold; ward. bath hed lequieds a firt nobor, we have's can to fordids me forim stul of them grabless wind mons to rewind, gays, buthit gner upon\n",
            "to til Roe sive ve thoughd pods.\n",
            "That thols, and tame, you seas, fightr, dors soo stae woung, tet, is of com staiser.\n",
            "\n",
            "Sher.'t Caren: out be het righs. your peaut wapes, in sie;\n",
            "This to my, this, twer one and med fea kin, we balwans fight rove Rorand don-hil man\n",
            "I word, seer no, awrad youghtrise:\n",
            "Stto hy pove, and hond an\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "generated_text = decode(model.generate(context, max_new_tokens=300)[0].tolist())\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generated Text (Self-Attention Model)\n",
        "\n",
        "Now that we have introduced self-attention, the output text, while still nonsensical in parts, contains more coherent words and partial sentences.\n",
        "\n",
        "You can see phrases like *\"your bes, you as:\"* and *\"Sher.'t Caren:\"* that—though random—are starting to look more like 16th-century English.\n",
        "\n",
        "By adjusting hyperparameters (e.g., more layers, larger embeddings), we can push this further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: perplexity: 4412.1, \n",
            "step 500: perplexity: 7.6, \n",
            "step 1000: perplexity: 6.7, \n",
            "step 1500: perplexity: 6.2, \n",
            "step 2000: perplexity: 6.0, \n",
            "step 2500: perplexity: 5.8, \n",
            "step 3000: perplexity: 5.7, \n",
            "step 3500: perplexity: 5.5, \n",
            "step 4000: perplexity: 5.5, \n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "batch_size = 32\n",
        "block_size = 6\n",
        "max_iters = 4001\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-3\n",
        "n_embed = 192\n",
        "n_heads = 3\n",
        "n_layer = 4\n",
        "dropout = 0.1\n",
        "#\n",
        "\n",
        "# model = SimpleBigramLanguageModel(vocab_size, n_embed, block_size)\n",
        "model = BigramLanguageModel(vocab_size, n_embed, block_size, n_layer, n_heads, dropout)\n",
        "\n",
        "# Setup training components\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "evaluator = Evaluator(model, train_loader, val_loader, vocab_size)\n",
        "trainer = Trainer(model, optimizer, train_loader, evaluator, max_iters, eval_interval)\n",
        "\n",
        "# Train the model\n",
        "final_losses = trainer.train()\n",
        "\n",
        "# 2.09 5.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling Up Further\n",
        "\n",
        "To demonstrate the impact of scaling, we adjust:\n",
        "- **`block_size`** to a smaller 6 here (just for demonstration)\n",
        "- **`n_embed`** to 192\n",
        "- **`n_layer`** to 4\n",
        "- **`n_heads`** to 3\n",
        "\n",
        "Even with these changes, perplexity drops into the mid-5 range, showing how deeper networks and bigger embeddings improve predictive power. In practice, you'd also raise the `block_size` to allow the model to see longer context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "NOR YORK:\n",
            "Love fring.\n",
            "\n",
            "LADY CAPULET:\n",
            "King Esell own so it not by\n",
            "here to I will love thee.\n",
            "\n",
            "SICINIUS:\n",
            "Action; porder.\n",
            "Who on the me proction.\n",
            "Now you the words my mercest fornight\n",
            "That a reconk lay to before-jasts a contern:\n",
            "O grath?\n",
            "\n",
            "COMINIUS:\n",
            "No fro to'th with your gruerm of mror now\n",
            "on yous: to't!' then't Cabsughtly senator\n",
            "Your chage\n",
            "Fausenue to the such in shive.\n",
            "Will bord, my exaincin, and M\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "generated_text = decode(model.generate(context, max_new_tokens=400)[0].tolist())\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Text Generation\n",
        "\n",
        "Our scaled-up model now produces text that hints at characters, place names, and partial phrases resembling Shakespeare.\n",
        "We see references to _\"LADY CAPULET\"_ and partial coherent lines that mimic stage directions or dialogues. While still not perfect English, it's closer in style to Shakespeare.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
